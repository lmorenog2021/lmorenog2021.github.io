---
layout: post
title: Task 6 - Balizas
---
# Objective of this task
The goal of this project is to build a robot that can locate itself inside a map using AprilTags as reference. if there are multiple tags avaliable, the robot must choose the closest (largest) one, and if no tags are to be found, the robot must use odometry to keep track of how he moved until finding a new tag.

The tags and their positions as well as the map should be previously known by the robot before execution of the program.

# Navigation
As the navigation is not the main focus of the excercise, I implemented a simple state machine that makes it so that the robot moves forward at a set speed until the LIDAR detects a wall in front of it. After finding the wall, the robot would turn in place for a determined amount of time, afterwich, it would continue to move forward. This implementation is more than enough to prove the efectivness of the self locating algorithm; which is the main focus of the task.

These are the global constants that regulate the behaviour of this state machine.

<img width="209" height="128" alt="image" src="https://github.com/user-attachments/assets/6c43a10d-7413-464d-983c-94fd0508898c" />


# Location without visible tags
The robot must be able to keep updating it's location despite losing a reference tag. To achieve this we use the odometry, while the nature of the simulator means that we constantly know the exact location of the robot, in the spirit of realism we need to make some compromises. The code has a secondary variable that tracks the *GetOdom2* value while a tag is in sight.

When the camera finds no tag, that same secondary value is compared to an actual *GetOdom2* value to measure the difference in position. This difference is supposed to emulate the measurements that a potentiometer connected to the wheels might have returned. With this emulated measurement we update the *estimated_pose* variable that represents the red car in the simulation.

<img width="318" height="255" alt="image" src="https://github.com/user-attachments/assets/138c9125-b9c3-4454-8bee-938af5c19075" />


# Location with visible tags
When AprilTags are visible, the system uses them as precise visual landmarks to directly compute the robot’s position in the environment. For each detected tag, a Perspective-n-Point (PnP) calculation estimates the camera’s position and orientation relative to that tag by comparing the known 3D geometry of the tag with its 2D appearance in the image. This works only because the camera’s intrinsic parameters (such as focal length and image center) are known, allowing the image measurements to be interpreted correctly in 3D space.

If multiple tags are detected at the same time, the code selects the one that appears largest in the image, since a larger tag usually means it is closer and provides a more accurate pose estimate. The chosen tag’s pose is then combined with the known position of that tag in the map and the fixed camera-to-robot relationship to compute the robot’s global position. This process produces an absolute, map-aligned pose that is significantly more reliable than motion-based estimates alone.

# Video
[![Balizas](https://img.youtube.com/vi/tk460CNLYIQ/0.jpg)](https://www.youtube.com/watch?v=tk460CNLYIQ)
