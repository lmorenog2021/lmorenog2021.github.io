---
layout: post
title: Task 6 - Balizas
---
# Objective of this task
The goal of this project is to build a robot that can locate itself inside a map using AprilTags as reference. if there are multiple tags avaliable, the robot must choose the closest (largest) one, and if no tags are to be found, the robot must use odometry to keep track of how he moved until finding a new tag.

The tags and their positions as well as the map should be previously known by the robot before execution of the program.

# Navigation
As the navigation is not the main focus of the excercise, I implemented a simple state machine that makes it so that the robot moves forward at a set speed until the LIDAR detects a wall in front of it. After finding the wall, the robot would turn in place for a determined amount of time, afterwich, it would continue to move forward. This implementation is more than enough to prove the efectivness of the self locating algorithm; which is the main focus of the task.

These are the global constants that regulate the behaviour of this state machine.
[Add Picture]

# Location without visible tags
The robot must be able to keep updating it's location despite losing a reference tag. To achieve this we use the odometry, while the nature of the simulator means that we constantly know the exact location of the robot, in the spirit of realism we need to make some compromises. The code has a secondary variable that tracks the *GetOdom2* value while a tag is in sight.

When the camera finds no tag, that same secondary value is compared to an actual *GetOdom2* value to measure the difference in position. This difference is supposed to emulate the measurements that a potentiometer connected to the wheels might have returned. With this emulated measurement we update the *estimated_pose* variable that represents the red car in the simulation.
[ADD Picutre]

# Location with visible tags
When AprilTags are visible, the robot switches to a **point-to-point (P2P) localization behavior**, meaning it directly computes its position from a known landmark instead of estimating motion over time. Each visible tag provides a full position and orientation estimate, and the system selects the **largest tag in the camera image**, since a larger apparent size usually means the tag is closer and its pose estimate is more reliable. This allows the robot to anchor itself precisely in the global map whenever visual information is available.

Accurate localization in this mode depends critically on knowing the **cameraâ€™s intrinsic parameters and its fixed position relative to the robot**. These values allow the system to correctly interpret how the tag appears in the image and translate that into a real-world position. If the camera model or mounting geometry is wrong, the P2P estimate will be consistently biased, even if the tag detection itself is perfect.

# Video
[ADD VIDEO]
